---
title: "OpenSAFELY demonstration"
author: "Will Hulme"
date: "14/10/2020"
output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, python.reticulate = FALSE)

```


## OpenSAFELY Introduction

OpenSAFELY is a new secure analytics platform for electronic health records in the NHS, created to deliver urgent research during the global COVID-19 emergency. It is now successfully delivering analyses across more than 24 million patients’ full pseudonymised primary care NHS records, with more to follow shortly.

All our analytic software is open for security review, scientific review, and re-use. OpenSAFELY uses a new model for enhanced security and timely access to data: we don’t transport large volumes of potentially disclosive pseudonymised patient data outside of the secure environments managed by the electronic health record software company; instead, trusted analysts can run large scale computation across near real-time pseudonymised patient records inside the data centre of the electronic health records software company. 

This document is intended as a short walkthrough of the OpenSAFELY platform. Please visit [docs.opensafely.org](https://docs.opensafely.org/en/latest/) for more comprehensive documentation, and [opensafely.org/](https://opensafely.org/) for any other info.


The examples in this document are all available in the [os-demo-research](https://github.com/opensafely/os-demo-research) github repository.

### Technical pre-requisites

OpenSAFELY maintains extremely high standards for data privacy, whilst ensuring complete computational and analytical transparency. As such there are some technical pre-requisites that users must satisfy to use the platform. These include installing and using git and python, though typically only a narrow set of actions are required and guide-rails are provided. These details can be read in the documentation pages, starting [here](https://docs.opensafely.org/en/latest/min_req/). The walkthrough assumes that the technical set-up has been completed.


### Key concepts

* A **study definition** specifies the patients you want to include in your study and defines the variables that describe them. Study definitions are defined as a Python script.  that relies heavily on an easily-readable functions API.
* The **cohort extractor** uses the study definition to create a dataset for analysis. This is either:
   * A dummy dataset used for developing and testing analysis code on the user's own machine. Users have control over the characteristics of each dummy variable, which are defined inside the study definition. 
   * A real dataset created from the OpenSAFELY database, used for the analysis proper. Real datasets never leave the secure server, only summary data and other outputs that are derived from them can be released (after disclosivity checks).
It also performs other useful tasks like importing codelists and generating _Measures_ (see below).
* A **Codelist** is a collection of clinical codes that define a particular condition, event or diagnosis.
* The **project pipeline** defines dependencies within the project's analytic code. For example `make_chart.R` depends on `process_data.R`, which depends on the study dataset having been extracted. This reduces redundancies by only running scripts that need to be run.
* The **job runner** runs the actions defined in the project pipeline using real data. 

### Workflow

For researchers using the platform, the OpenSAFELY workflow is typically as follows:

1 Create a git repository from the template provided
2 Specify the dataset(s) that you want to extract from the database:
  * create the Study Definition(s) that defines the patient population (dataset rows) and variables (dataset columns), and their expected distributions for use in dummy data
  * identify (or create) the codelists required by the study definition, as hosted by codelists.opensafely.org, and import them to the repo
3 Generate the dummy data
4 Write and test analysis scripts using the dummy dataset in R, Stata, or Python. This will include:
  * importing and processing the dataset(s) created by the cohort extractor
  * importing any other external files needed for analysis
  * generating analysis outputs like tables and figures
  * generating log files to debug the scripts when run on the real data
5 Create a project pipeline to run steps 3 and 4
6 Run the analysis scripts on the real data via the job runner
7 Assuming no errors, check the output for disclosivity and release the outputs where appropriate

Steps 2-5 can all be progressed on your local machine without accessing the real data. These steps are iterative and should be accompanied by frequent, well-structured git commits, with code reviews where appropriate. These steps are automatically tested against dummy data every time a new version of the repository is saved ("pushed") to GitHub. 


Let's start with a simple example.


## Example 1: STP patient population

This example introduces study definitions, dummy data, and project pipelines. We're going to use OpenSAFELY to find out how many patients are registered at a TPP practice within each STP (Sustainability and Transformation Partnership) on 1 January 2020. 


#### Study definition
We start by defining the study definition. The entire script, called [`study_definition_1_stppop.py`](https://github.com/opensafely/os-demo-research/blob/master/analysis/study_definition_1_stppop.py) looks like this:

```{python,  eval=FALSE, class.source = "fold-show"}

## LIBRARIES

# cohort extractor
from cohortextractor import (StudyDefinition, patients)

# dictionary of STP codes (for dummy data)
from dictionaries import dict_stp

# set the index date
index_date = "2020-01-01"

## STUDY POPULATION

study = StudyDefinition(

    # this line sets some default expecetations for the dummy data
    default_expectations = {
        "date": {"earliest": index_date, "latest": "today"}, # date range for simulated dates
    },

    # This line defines the study population
    population = patients.registered_as_of(index_date),

    # this line defines the stp variable we want to extract
    stp = patients.registered_practice_as_of(
        index_date,
        returning="stp_code",
        return_expectations={
            "category": {"ratios": dict_stp},
        },
    ),
)


```

Let's break it down:

```
from cohortextractor import (StudyDefinition, patients)
```

This imports the required functions from the OpenSAFELY `cohortextractor` library, that you will have previously installed.

```
from dictionaries import dict_stp
```
This imports a dictionary of STP codes for creating the STP dummy data.

```
index_date = "2020-01-01"
```
This defines the registration date that we're interested in.

We then use the `StudyDefinition()` function to define the cohort population and the variables we want to extract. 

```
default_expectations = {
    "date": {"earliest": index_date, "latest": "today"}, # date range for simulated dates
    "rate": "uniform",
},
```
This defines the default expectations which are used to generate dummy data. This just says that we expect date variables to be uniformlly distributed between the index date and today's date. 

```
population = patients.registered_as_of(index_date)
```
This says that we want to extract information only for patients who were registered at a practice on the 1 January 2020. There will be one row for each of these patients in the extracted dataset. Note that `population` is a reserved variable name for `StudyDefinition` which specifies the study population &mdash; we don't have to do any additional filtering/subsetting on this variable. 

```
stp = patients.registered_practice_as_of(
  index_date,
  returning="stp_code",
  return_expectations={
    "incidence": 0.99,
    "category": {"ratios": dict_stp},
  },
)
```

This says we want to extract the STP for each patient (or more strictly, the STP of each patients' practice). Here we also use the `returning_expectations` argument, which specifies how the `stp` variable will be distributed in the dummy data. option, The `"incidence": 0.99"` line says that we expect an STP code to be available for 99\% of patients. The `"category": {"ratios": dict_stp}` line uses the pre-defined STP dictionary `dict_stp` (imported earlier) to define the expected distribution of STPs. This is currently set to a uniform distribution, so that each STP is equally likely to appear in the dataset.


This study definition uses two in-built variable extractor functions in OpenSAFELY's `patients` library, `patients.registered_as_of()` and `patients._practice_as_of()`. There are many more such functions, like `patients.age()`, `patients.with_these_clinical_events()`, and `patients.admitted_to_icu()`, which are all documented here. 


#### Dummy data

Now that we've defined the study cohort, we can generate a dummy dataset. Assuming you have the correct technical set-up on your local machine, this is simply a case of submitting the following command in a terminal that can find `cohortextractor` (you can use `cohortextractor --help` to find out more about this command, the available options, defaults, etc.):

```
cohortextractor generate_cohort --study-definition study_defintion_1_stppop.py --expectations-population 10000 --output-dir=output/cohorts
```

This will create a file `input_1_stppop.csv` in the `/output/cohorts/` folder with `10000` rows. 

#### Develop the analysis scripts

We now have a dummy dataset so we can begin to develop and test our analysis code. FOr this example, we just want to import the dataset, count the number of STPs, and output to a file. These steps are outlined code-block below.

First we import and process the data:
```{r stppop}
## import libraries
library('tidyverse')
library('sf')
## import data
df_input <- read_csv(
  here::here("output", "cohorts", "input_1_stppop.csv"), 
  col_types = cols(
    patient_id = col_integer(),
    stp = col_character()
  )
)
# import the STP shapefile for the map
# from https://openprescribing.net/api/1.0/org_location/?format=json&org_type=stp
# not importing directly from the URL because no access on the server, so have copied into the repo
sf_stp <- st_read(here::here("lib", "STPshapefile.json"))

# count STP for each registered patient
df_stppop = df_input %>% count(stp, name='registered')

#combine with STP geographic data
sf_stppop <- sf_stp %>% 
  left_join(df_stppop, by = c("ons_code" = "stp")) %>%
  mutate(registered = if_else(!is.na(registered), registered, 0L))
```

Then we create a map:
```{r stppop.map}
plot_stppop_map <- sf_stppop %>%
ggplot() +
  geom_sf(aes(fill=registered), colour='black') +
  scale_fill_gradient(limits = c(0,NA), low="white", high="blue")+
  labs(
    title="TPP-registered patients per STP",
    subtitle= "as at 1 January 2020",
    fill = NULL)+
  theme_void()+
  theme(
    legend.position=c(0.1, 0.5)
  )
plot_stppop_map
```

Or a bar chart if you prefer:
```{r stppop.bar}
plot_stppop_bar <- sf_stppop %>%
  mutate(
    name = forcats::fct_reorder(name, registered, median, .desc=FALSE)
  ) %>%
  ggplot() +
  geom_col(aes(x=registered/1000000, y=name, fill=registered), colour='black') +
  scale_fill_gradient(limits = c(0,NA), low="white", high="blue", guide=FALSE)+
  labs(
    title="TPP-registered patients per STP",
    subtitle= "as at 1 January 2020",
    y=NULL,
    x="Registered patients\n(million)",
    fill = NULL)+
  theme_minimal()+
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    plot.title.position = "plot",
    plot.caption.position =  "plot"
  )
plot_stppop_bar
```


This is just dummy data but we'll be able to easily rerun the code above on the real dataset to see how this map looks for real. For this, we use the OpenSAFELY Job Runner. 

#### Job Runner and Project Pipelines

The OpenSAFELY Job Runner is used to run analysis scripts on the real data. This includes extracting data from the OpenSAFELY database as defined in a study definition, and running R or Stata scripts on that data. To do this, we define the Project Pipeline. This is a set of actions written in a file called `project.yaml`. This is best demonstrated with an example.

A simple `project.yaml` for this study looks like this:

```yaml

version: '3.0'

expectations:
  population_size: 10000

actions:

  generate_cohort_stppop:
    run: cohortextractor:latest generate_cohort --study-definition study_definition_1_stppop --output-dir=output/cohorts
    outputs:
      highly_sensitive:
        cohort: output/cohorts/input_1_stppop.csv

  plot_stppop:
    run: r:latest analysis/1-plot-stppop.R
    needs: [generate_cohort_stppop]
    outputs:
      moderately_sensitive:
        log: output/logs/log-1-plot-stppop.txt
        figure1: output/plots/plot_stppop_map.png
        figure2: output/plots/plot_stppop_bar.png
        
        
  run_all:
    needs: [plot_stppop]
    # In order to be valid this action needs to define a run commmand and
    # some output. We don't really care what these are but the below seems to
    # do the trick.
    run: cohortextractor:latest --version
    outputs:
      moderately_sensitive:
        whatever: project.yaml
```

The file begins with some housekeeping that specifies which version to use and the size of the dummy dataset used for testing.

The main part of the yaml file is the `actions` section. In this example we have three actions:
* `generate_cohort_stppop` is the action to extract the dataset from the database. The `run` section gives the extraction command (`generate_cohort`), which study definition to use (`--study-definition study_definition_1_stppop`) and where to put the output (`--output-dir = output/cohorts`). The `outputs` section declares that the expected output is `highly_sensitive` and shouldn't be considered for release.
* `plot_stppop` is the action that runs the R script. The `run` section tells it which script to run. The `needs` section says that this action cannot be run without having first run the `geenrate_cohort_stppop` action. The `outputs` sections declares that the expected outputs (a log file for debugging, and the two plots) are `moderately_sensitive` and should be considered for release after disclosivity checks.
* `run_all` is an action necessary for testing. By setting the `needs` section to include all actions, it will run all other actions and test that they can complete successfully.

Now that the `project.yaml` pipeline is defined, it can be run via the Job Runner at `http://jobs.opensafely.org/jobs/` as follows:

* create a workspace for the repo + branch, and select the appropriate backend (in this case `TPP`)
* click `add job` and select which actions to run, then submit.

This will create the required outputs on the server, which can be reviewed on the serrver and released via github if they are deemed non-disclosive (there are detailed reviewing guidelines for approved researchers). 

#### Real results

Once resuts have been released we can view them on github. The map looks like this:


![STP map](/released-ouput/plots/plot_stppop_map.png "registration count by STP")

and the bar chart looks like this:

![STP bar](/released-ouput/plots/plot_stppop_bar.png "registration count by STP")

Here we're just linking to the released `.png` files in the repo. We could have also released the underlying STP count data and developed an analysis script directly on this data, provided it is non-disclosive. However, typically only the most high-level aggregated datasets are suitable for public release. 

## Example 2: Covid versus non-covid deaths

This example introduces codelists. We're going to use OpenSAFELY to look at the frequency of covid-related deaths compared with non-covid deaths between 1 January 2020 and 30 September 2020, and see how this differs by age and sex. For this, we'll need death dates for anyone in the database who died during the period of interest, and we'll need a way to identify whether these deaths were covid-related or not.

#### Study definition

The study definition for this task is available [here](https://github.com/opensafely/os-demo-research/blob/master/analysis/study_definition_2_deaths.py). 

As before, we first import libraries and dictionaries. This time we also import the codelist for identifying covid-related deaths. This uses data from death certificates, which are coded using ICD-10 codes. The covid codes in this system are `U071` and `U072`, and have been collected in a codelist at [codelists.opensafely.org](https://codelists.opensafely.org/codelist/opensafely/covid-identification/2020-06-03). To import the codelists, put the codelist url in the `codelists/codelists.txt` file in the repo, then run the following command:

```
cohortextractor update_codelists
```

This will create a `.csv` for each codelist, which are imported to the study definition using

```
codes_ICD10_covid = codelist_from_csv(
    "codelists/opensafely-covid-identification.csv", 
    system="icd10", 
    column="icd10_code"
)
```

Then as before, we define the cohort population and the variables we want to extract within `StudyDefinition()`. The variables we extract this time are `age`, `sex`, `date_death`, and `death_category`. These use some new extractor functions and expectation definitions, whose details can be found in the documentation [here](https://docs.opensafely.org/en/latest/study_def_intro/).

#### Results

Once again, we use the dummy data to develop the analysis script then create a `project.yaml` to run the entire study on the real data. The analysis script is available [here](https://github.com/opensafely/os-demo-research/blob/master/analysis/2-plot-deaths.R). This script is run by the Job Runner via the `project.yaml` file, then the outputs are reviewed in the server and released via github. The final graph looks like this:

![covid deaths](/released-ouput/plots/plot_deaths.png "covid-related deaths")

Here you can see the familiar covid mortality bump during the first wave of the pandemic. There is also a bump in non-covid deaths, suggesting that identification of covid-related deaths may not be 100\% sensitive. 

## Example 3: Primary care activity throughout the pandemic.

In our final example, we introduce the Measures framework. This enables the extraction of multiple study cohorts each covering different time periods, and calculates a set of statistics for each period. In this example we'll look at the frequency of cholesterol and INR (International Normalised Ratio, which measures how long it takes for blood to clot) measurements recorded in the Primary Care record. We'll look at this by practice and by STP.


#### Study Definition

The entire study definition is available [here](https://github.com/opensafely/os-demo-research/blob/master/analysis/study_definition_3_activity.py), and we'll pick out some key parts. 

```{python, eval=FALSE, class.source = "fold-show"}
population = patients.satisfying(
        """
        (age >= 18 AND age < 120) AND
        (NOT died) AND
        (registered)
        """,
        
        died = patients.died_from_any_cause(
		    on_or_before=index_date,
		    returning="binary_flag"
	    ),
        registered = patients.registered_as_of(index_date),
        age=patients.age_as_of(index_date),
    )
```
The population declaration says that for each period, we want the set of adult patients who are both alive and registered at the index date.



```{python, eval=FALSE, class.source = "fold-show"}
cholesterol = patients.with_these_clinical_events(
        codes_cholesterol,
        returning = "number_of_episodes",
        between = ["index_date", "index_date + 1 month"],
        return_expectations={
            "int": {"distribution": "normal", "mean": 2, "stddev": 0.5}
        },
    ),

    inr = patients.with_these_clinical_events(
        codes_inr,
        returning = "number_of_episodes",
        between = ["index_date", "index_date + 1 month"],
        return_expectations={
            "int": {"distribution": "normal", "mean": 3, "stddev": 0.5}
        },
    ),
```
Then we want to extract the number of cholesterol- or INR-measurement "episodes" recorded during the month beginning the index date. The `codes_cholesterol` and the `codes_inr` codelists are defined similarly to the `codes_ICD10_covid` in example 2. Using expectations, we say the dummy values for these variables will be a low-valued integer.



```{python, eval=FALSE, class.source = "fold-show"}
measures = [
    Measure(
        id="cholesterol_practice",
        numerator="cholesterol",
        denominator="population",
        group_by="practice"
    ),
    Measure(
        id="cholesterol_stp",
        numerator="cholesterol",
        denominator="population",
        group_by="stp"
    ),
    Measure(
        id="inr_practice",
        numerator="inr",
        denominator="population",
        group_by="practice"
    ),
    Measure(
        id="inr_stp",
        numerator="inr",
        denominator="population",
        group_by="stp"
    ),
]
```

Finally, we define the measures that we want to calculate. Here we want four measures, one for each combination of cholesterol/inr and practice/STP. For more details on Measures, see the documentation [here](https://docs.opensafely.org/en/latest/measures/). 

#### `generate_measures`

As before, we generate dummy data using the `generate_cohort` command in the `cohortextractor`, but this time, we include a `--index-date-range` option so that it extracts a new cohort for each date specified, as follows:

```
cohortextractor generate_cohort --study-definition study_definition_3_activity --expectations-population 10000 --index-date-range "2020-01-01 to 2020-09-01 by month"
```

Here we go from 1 January 2020 to 1 September 2020 in monthly increments. These dates are passed to the `index_date` variable in the study definition. This will produce a set of `input_3_activity_<date>.csv` files, each with `10000` rows of dummy data.

An additional step is now needed to generate the measures. This is done as follows:

```
cohortextractor generate_measures --study-definition study_definition_3_activity
```

which will produce a set of files of the form `measure_<id>.csv`. 

#### Real data

Now we have the extracted outputs we can develop our analysis script. The analysis script for this can be found [here](https://github.com/opensafely/os-demo-research/blob/master/analysis/3-plot-activity.R). 


Let's look at the number of measurements in each STP for cholesterol:

![cholesterol stp actual](/released-ouput/plots/plot_each_cholesterol_stp.png)

and for INR:

![inr stp actual](/released-ouput/plots/plot_each_cholesterol_inr.png)

Clearly there is a substantial dip in activity which corresponds neatly to the first pandemic wave.


We can also look at deciles of measurement activity for each GP practice for cholesterol:

![cholesterol practice quantile](/released-ouput/plots/plot_quantiles_cholesterol_stp.png)

and for INR:

![inr practice quantile](/released-ouput/plots/plot_quantiles_cholesterol_inr.png)






## Future development

* Better dummy data, respecting between-variable dependencies and pre-loaded dictionaries
* Fast patient matching algorithms for example for case-control studies
* 
